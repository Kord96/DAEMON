{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_model (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Bai61Tf5EEXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<big> constants, imports"
      ]
    },
    {
      "metadata": {
        "id": "vbWnme9dAxfw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5c536689-38b6-40db-9b0a-e53a87f74a97"
      },
      "cell_type": "code",
      "source": [
        "#constants\n",
        "filename=\"chatbot_model\"\n",
        "TLD = \"/content/drive/My Drive/TensorBoard_logs/{}/\".format(filename)\n",
        "bash_TLD = \"/content/drive/My\\ Drive/TensorBoard_logs/{}/\".format(filename)\n",
        "data_folder = 'drive/My Drive/Data/RC/'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ol0vXp-Ijdds",
        "outputId": "57aae8b9-572e-484d-f1ee-3ab8c4fdc37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#import utils\n",
        "import os\n",
        "from time import time\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Flatten, Input, LSTM, Dense, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Lambda\n",
        "from keras.models import load_model\n",
        "from keras.backend import argmax\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import signature_constants\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "#import keras.layers.LSTM\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "start_token = 'starttoken'\n",
        "end_token = 'endtoken'\n",
        "\n",
        "number_of_classes = 22\n",
        "\n",
        "vocab_size = 40000\n",
        "\n",
        "\n",
        "embedding_vector_dim = 300\n",
        "\n",
        "max_sequence_length = 20\n",
        "\n",
        "max_decoder_seq_length = 25\n",
        "\n",
        "\n",
        "chatbot_ds_file = 'combined1.txt'\n",
        "\n",
        "# path = ''\n",
        "# Glove_path = 'glove.6B.'+str(embedding_vector_dim)+'d.txt'\n",
        "# mount google drive\n",
        "path = '/content/drive/My Drive/Data/chatbot_general_data/'\n",
        "# Glove_path = '/content/drive/My Drive/Data/Glove/glove.6B.'+str(embedding_vector_dim)+'d.txt'\n",
        "word2vec_path = '/content/drive/My Drive/Data/fastText/wiki-news-300d-1M-subword.vec'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sj8YUplpA1tI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#don't worry about this, this is just for tensorboard data\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.python.eager import context\n",
        "\n",
        "class TrainValTensorBoard(TensorBoard):\n",
        "    def __init__(self, log_dir=\"/content/drive/My Drive/TensorBoard_logs/{}\".format(filename), **kwargs):\n",
        "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
        "        training_log_dir = os.path.join(log_dir, 'training')\n",
        "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
        "\n",
        "    def set_model(self, model):\n",
        "        if context.executing_eagerly():\n",
        "            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
        "        else:\n",
        "            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
        "        super(TrainValTensorBoard, self).set_model(model)\n",
        "\n",
        "    def _write_custom_summaries(self, step, logs=None):\n",
        "        logs = logs or {}\n",
        "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
        "        if context.executing_eagerly():\n",
        "            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "                for name, value in val_logs.items():\n",
        "                    tf.contrib.summary.scalar(name, value.item(), step=step)\n",
        "        else:\n",
        "            for name, value in val_logs.items():\n",
        "                summary = tf.Summary()\n",
        "                summary_value = summary.value.add()\n",
        "                summary_value.simple_value = value.item()\n",
        "                summary_value.tag = name\n",
        "                self.val_writer.add_summary(summary, step)\n",
        "        self.val_writer.flush()\n",
        "\n",
        "        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
        "        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
        "        self.val_writer.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-sSHezMKJBy",
        "colab_type": "code",
        "outputId": "86a58663-c297-4a69-b51f-9dbca54f5280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "def tensorBoard():\n",
        "  #LOG_DIR = './content/drive/My Drive/TensorBoard_logs/{}'.format(filename)\n",
        "  LOG_DIR = \"./log\"\n",
        "  !rsync -a $bash_TLD \"./log\"\n",
        "  \n",
        "  tensorboard = TensorBoard(log_dir=LOG_DIR)\n",
        "  print('LOG_DIR:',LOG_DIR)\n",
        "  get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        "   )\n",
        "  get_ipython().system_raw('./ngrok http 6006 &')\n",
        "  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "#use this link to enter tensorBoard\n",
        "tensorBoard()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOG_DIR: ./log\n",
            "https://97727985.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xguWfCDkFGRG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<big> loading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A_7nv9nsxm72",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input training data to chatbot           sample line: question+'\\t'+answer\n",
        "def input_chatbot_data(fileName):\n",
        "  '''\n",
        "  Args:\n",
        "    fileName (str): file to load\n",
        "  Return:\n",
        "    init (list): list of questions\n",
        "    ip_res (list): list of answers pre-padded with start_token\n",
        "    op_res (list): list of answers post-padded woth end_token\n",
        "  '''\n",
        "  init = []\n",
        "  ip_res = []\n",
        "  op_res = []\n",
        "  with open(fileName, encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      lineSplit = line.split('\\t')\n",
        "      Q = lineSplit[0]\n",
        "      if(len(Q) >= 2 and (Q[-1] == '.' or Q[-1] == '?' or Q[-1] == '!')): Q = Q[:-1] + ' ' + Q[-1]\n",
        "      init.append(Q)\n",
        "      \n",
        "      Q = lineSplit[1]\n",
        "      if(len(Q) >= 2 and (Q[-2] == '.' or Q[-2] == '?' or Q[-2] == '!')): Q = Q[:-2] + ' ' + Q[-2]\n",
        "      ip_res.append(start_token + ' ' + Q)\n",
        "      op_res.append(Q + ' ' + end_token)\n",
        "  return init,ip_res,op_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0dRVvApBrnB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_df(name):\n",
        "  path = data_folder + name\n",
        "  df.to_csv(path_or_buf= data_folder + name, sep='&')\n",
        "\n",
        "def load_df(name):\n",
        "  return pd.read_csv(data_folder + name, sep='&',index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gsf9mo_bBze5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = load_df('20k_df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNnM0isppQEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_tokens(data):\n",
        "  init = data['comment']\n",
        "  ip_res = start_token + ' ' + data['reply']\n",
        "  op_res = data['reply'] + ' '+ end_token\n",
        "  \n",
        "  return init,ip_res,op_res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMqOZWaYEPTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<big> embeddings\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XUTzSH-uKEqj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tokenizer(X):\n",
        "    ##TOKENIZE ON SPACES ONLY? HOW WILL IT AFFECT t.texts_to_sequences(df)\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(X)\n",
        "    return t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Unz67RUHw6Ac",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = get_tokenizer(index_to_word)\n",
        "# print(t.word_index[\"the\"])\n",
        "\n",
        "vocab_size = len(t.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_vector_dim))\n",
        "for word, i in t.word_index.items():\n",
        "  if(i==0): print('oh no')\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    \n",
        "# flag = True\n",
        "# for index in range(1,total_vocab_size):\n",
        "#     flag = flag and ( t.index_word[index] == index_to_word[index] )\n",
        "    \n",
        "\n",
        "   \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_wDDAba7nobP",
        "outputId": "a2bc2852-1a94-4208-ae99-000188243403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('Indexing word vectors.')\n",
        "\n",
        "#embedding_matrix = np.zeros((total_vocab_size, embedding_vector_dim))\n",
        "\n",
        "index_to_word = []\n",
        "word_to_index = {}\n",
        "embeddings_index = {}\n",
        "f = open(word2vec_path, encoding='utf-8')\n",
        "for index, line in enumerate(f):\n",
        "    if index < vocab_size :\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        index_to_word.append(word)\n",
        "        word_to_index[word] = index\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "        #embedding_matrix[index + 1] = coefs\n",
        "    else:\n",
        "      break\n",
        "f.close()\n",
        "\n",
        "embeddings_index[start_token] = np.zeros(embedding_vector_dim) + 0.0015\n",
        "index_to_word.append(start_token)\n",
        "\n",
        "embeddings_index[end_token] = np.zeros(embedding_vector_dim) - 0.0015\n",
        "index_to_word.append(end_token)\n",
        "\n",
        "#print('Found %s word vectors.' % len(embedding_matrix))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uzVQpI89BS8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "664c83a7-2d39-4865-94cc-3b7f6af8301b"
      },
      "cell_type": "code",
      "source": [
        "##adding embbeding into tensorBoard\n",
        "\n",
        "def embeddings_to_TLD():\n",
        "  import numpy as np\n",
        "  embeddings_vectors = np.stack(embedding_matrix)\n",
        "  emb = tf.Variable(embeddings_vectors, name='word_embeddings')\n",
        "  init_op = tf.global_variables_initializer()\n",
        "  saver = tf.train.Saver()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    save_path = saver.save(sess,\"/content/drive/My Drive/TensorBoard_logs/{}/model.ckpt\".format(filename))\n",
        "    print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "  words = '\\n'.join(index_to_word)\n",
        "  with open(os.path.join(\"/content/drive/My Drive/TensorBoard_logs/{}\".format(filename), 'metadata.tsv'), 'w') as f:\n",
        "     f.write(words)\n",
        "embeddings_to_TLD()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/drive/My Drive/TensorBoard_logs/chatbot_model/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rkTcSCWepu66",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#device_lib.list_local_devices()\n",
        "# def download_file(): \n",
        "#   !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
        "#   !mv -v /content/*.vec '/content/drive/My Drive/Data/fastText'\n",
        "  \n",
        "# download_file()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2bXE-G4Zw6Aw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #outputs numerically tokenized words\n",
        "\n",
        "# def preprocessInputSentences(sentences):\n",
        "#     tokenized_sentences = t.texts_to_sequences(sentences)\n",
        "#     print(tokenized_sentences)\n",
        "#     print(type(tokenized_sentences))\n",
        "#     #list of lists to numpy array of arrays\n",
        "#     tokenized_sentences = np.array([np.array(sentence) for sentence in tokenized_sentences])\n",
        "#     tokenized_sentences = tokenized_sentences[tokenized_sentences != 0]\n",
        "#     tokenized_sentences -= 1\n",
        "#     print(tokenized_sentences)\n",
        "#     print(type(tokenized_sentences))\n",
        "#     tokenized_sentences =  [element for element  in array for array in tokenized_sentences.tolist()]\n",
        "#     print(tokenized_sentences)\n",
        "#     print(type(tokenized_sentences[0][0]))\n",
        "#     return pad_sequences(tokenized_sentences, maxlen = max_sequence_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSJRrzLfFQDj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<big> padding, model and training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "svVowdKCw6A1",
        "outputId": "fc6441dd-11ee-41fe-9593-08a76563a6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# '''\n",
        "# initiation = [\"hi how are you\", \"whats your name\", \"how much is this watch\"]\n",
        "# input_response = [start_token+\" i am fine thank you\", start_token+\" my name is fadi\", start_token+\" its so expensive you cant afford\"]\n",
        "# output_response = [\"i am fine thank you \"+end_token, \"my name is fadi \"+end_token,\"its so expensive you cant afford \"+end_token]\n",
        "# '''\n",
        "\n",
        "#initiation,input_response,output_response = input_chatbot_data(path + chatbot_ds_file)\n",
        "initiation,input_response,output_response = add_tokens(df)\n",
        "\n",
        "\n",
        "encoder_input_data = pad_sequences(t.texts_to_sequences(initiation), maxlen = max_sequence_length, padding='post')\n",
        "\n",
        "decoder_input_data = pad_sequences(t.texts_to_sequences(input_response), maxlen = max_sequence_length, padding='post')\n",
        "\n",
        "decoder_target_data = pad_sequences(t.texts_to_sequences(output_response), maxlen = max_sequence_length, padding='post')\n",
        "\n",
        "#decoder_target_data = to_categorical((decoder_target_data), num_classes = total_vocab_size + 1)\n",
        "\n",
        "# print(\"\")\n",
        "# print(to_categorical([-1,-1,3,0]))\n",
        "encoder_input_data_train, encoder_input_data_validation, decoder_input_data_train, decoder_input_data_validation, decoder_target_data_train, decoder_target_data_validation = train_test_split(encoder_input_data, decoder_input_data, decoder_target_data, test_size = 0.1, random_state = 42)\n",
        "\n",
        "print(len(encoder_input_data_train))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hbmPVVD68JYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "latent_dim = 200\n",
        "\n",
        "bottleneck_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mFJA0bjAj7db",
        "outputId": "c8aa3f4d-2ddf-4174-cd20-6da3d7483aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.Session(config = config)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
        "embeddingLayer = Embedding(vocab_size, embedding_vector_dim, weights=[embedding_matrix], trainable = False,name=\"EmbeddingLayer\")\n",
        "embeddingForInput = embeddingLayer(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True,name=\"LSTM_Encoder\")\n",
        "encoder_outputs, state_h, state_c = encoder(embeddingForInput)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "# decoder_inputs = Input(shape=(None, total_vocab_size))\n",
        "# relu_decoder_input_bottleneck_layer = Dense(bottleneck_size, activation='relu')\n",
        "# relu_decoder_input_bottleneck_layer_output = relu_decoder_input_bottleneck_layer(decoder_inputs)\n",
        "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
        "embeddingForOutput = embeddingLayer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,name=\"LSTM_Decoder\")\n",
        "decoder_outputs, _, _ = decoder_lstm(embeddingForOutput,\n",
        "                                     initial_state=encoder_states)\n",
        "relu_decoder_output_bottleneck_layer = Dense(bottleneck_size, activation='relu')\n",
        "relu_decoder_output_bottleneck_layer_output = relu_decoder_output_bottleneck_layer(decoder_outputs)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(relu_decoder_output_bottleneck_layer_output)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "writer = tf.summary.FileWriter(TLD, graph=sess.graph)\n",
        "print(model.summary())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "EmbeddingLayer (Embedding)      (None, None, 300)    9576900     encoder_inputs[0][0]             \n",
            "                                                                 decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_Encoder (LSTM)             [(None, 200), (None, 400800      EmbeddingLayer[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_Decoder (LSTM)             [(None, None, 200),  400800      EmbeddingLayer[1][0]             \n",
            "                                                                 LSTM_Encoder[0][1]               \n",
            "                                                                 LSTM_Encoder[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 100)    20100       LSTM_Decoder[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 31923)  3224223     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 13,622,823\n",
            "Trainable params: 4,045,923\n",
            "Non-trainable params: 9,576,900\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f5aTf9kofedj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(encoder_input, decoder_input, decoder_output, batch_size):\n",
        "\n",
        "    while True:\n",
        "        batch_start = 0\n",
        "        total_length = len(encoder_input)\n",
        "        while batch_start < total_length:\n",
        "          batch_end = min(batch_start + batch_size, total_length)\n",
        "          yield [encoder_input[batch_start : batch_end], decoder_input[batch_start : batch_end]], to_categorical(decoder_output[batch_start : batch_end], num_classes = vocab_size)\n",
        "          batch_start += batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdxfXjvU8JYx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 125\n",
        "epochs = 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1NJTAk0Yw6A7",
        "outputId": "59f2b5a2-8c93-4c34-94e6-6ab30c18f412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "model.compile(optimizer='adam', loss=losses.categorical_crossentropy, metrics=['categorical_accuracy'])\n",
        "#model.fit([encoder_input_data_train, decoder_input_data_train], to_categorical(decoder_target_data_train, num_classes = vocab_size),\n",
        "#          batch_size = batch_size, epochs=epochs)\n",
        "\n",
        "#writer = tf.summary.FileWriter('./log', graph=sess.graph)\n",
        "\n",
        "model.fit_generator(generator(encoder_input_data_train, decoder_input_data_train, decoder_target_data_train, batch_size),\n",
        "                    steps_per_epoch = int(len(encoder_input_data_train) / batch_size),                 \n",
        "                    epochs = epochs,\n",
        "                    validation_data = generator(encoder_input_data_validation, decoder_input_data_validation, decoder_target_data_validation, batch_size),\n",
        "                    validation_steps = int(len(encoder_input_data_validation) / batch_size), callbacks=[TrainValTensorBoard(write_graph=False)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "144/144 [==============================] - 70s 487ms/step - loss: 5.2124 - categorical_accuracy: 0.4119 - val_loss: 4.1302 - val_categorical_accuracy: 0.4323\n",
            "Epoch 2/3\n",
            "144/144 [==============================] - 65s 451ms/step - loss: 3.9600 - categorical_accuracy: 0.4392 - val_loss: 3.8577 - val_categorical_accuracy: 0.4649\n",
            "Epoch 3/3\n",
            "144/144 [==============================] - 65s 452ms/step - loss: 3.8128 - categorical_accuracy: 0.4626 - val_loss: 3.7922 - val_categorical_accuracy: 0.4772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc004a1bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "W9jUXpNK8clm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_savedmodel(model, export_path):\n",
        "  \"\"\"Convert the Keras HDF5 model into TensorFlow SavedModel.\"\"\"\n",
        "\n",
        "  builder = saved_model_builder.SavedModelBuilder(export_path)\n",
        "  outputs_dict =  {'output' + str(index) :  output for index, output in enumerate(model.outputs)}\n",
        "  signature = predict_signature_def(\n",
        "      inputs={'encoder_input': model.inputs[0], 'input_token': model.inputs[1] }, outputs = outputs_dict)\n",
        "\n",
        "  with K.get_session() as sess:\n",
        "    builder.add_meta_graph_and_variables(\n",
        "        sess=sess,\n",
        "        tags=[tag_constants.SERVING],\n",
        "        signature_def_map={\n",
        "            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature\n",
        "        })\n",
        "    builder.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNky9XUd8JY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss, accuracy = model.evaluate([encoder_input_data_test, decoder_input_data_test], to_categorical(decoder_target_data_test), batch_size = len(encoder_input_data_test))\n",
        "# print('test Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svd6KgybFmE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def myfunction(x):\n",
        "  return x[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H65yAQe0w6BC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "argmax_layer =  Lambda(argmax)\n",
        "\n",
        "current_state = encoder_states\n",
        "\n",
        "input_token = Input(shape=(None,), name = \"input_token\")\n",
        "current_input = input_token\n",
        "decoder_outputs = []\n",
        "\n",
        "for i in range(max_sequence_length):\n",
        "    embedding = embeddingLayer(current_input)\n",
        "    decoder_output, h, c = decoder_lstm(embedding, initial_state = current_state)\n",
        "    decoder_output = relu_decoder_output_bottleneck_layer(decoder_output)\n",
        "    decoder_output = decoder_dense(decoder_output)\n",
        "    decoder_output = argmax_layer(decoder_output)\n",
        "    current_input = decoder_output\n",
        "    current_state = h, c\n",
        "    decoder_outputs.append(decoder_output)\n",
        "\n",
        "\n",
        " \n",
        "      \n",
        "\n",
        "model_p = Model(inputs = [encoder_inputs, input_token] , outputs = decoder_outputs)\n",
        "\n",
        "#this looks wrong, check tensorBoard\n",
        "writer = tf.summary.FileWriter('./log', graph=sess.graph)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5PhB6ZDP4tqK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_p.inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "stGF41Rb8JY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#model.save('full_chatbot_model_test1_dataset.h5')\n",
        "#K.set_learning_phase(0)\n",
        "#to_savedmodel(model_p, '')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FPxFNNkrw6BI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#encoder_model = load_model('encoder_model.h5')\n",
        "#decoder_model = load_model('decoder_model.h5')\n",
        "#print(encoder_model.summary())\n",
        "# print(decoder_model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XFn60e_Sw6BP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  sentence = ''\n",
        "  end_token_index = t.word_index[end_token]\n",
        "  start_token_index = np.full((1, 1), fill_value = t.word_index[start_token])\n",
        "  for element in model_p.predict([input_seq, start_token_index]):\n",
        "    element = element[0, 0]\n",
        "    if (element == 0 or element == end_token_index):\n",
        "      break\n",
        "    sentence += ' ' +  t.index_word[element]\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mr7KXy5Lw6BV",
        "outputId": "9330d121-83fe-4b08-a134-172af091a92b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(decode_sequence(pad_sequences(t.texts_to_sequences([\"hi\"]), maxlen = max_sequence_length, padding='post')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " i think you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uUdCzZYgivao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(pad_sequences(t.texts_to_sequences([\"hi\"]), maxlen = max_sequence_length, padding='post'))\n",
        "print(t.word_index[start_token])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oQakVEWy4f26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #loading glove\n",
        "# #note: should be dumbed into drive to not download it everytime\n",
        "# def download(): \n",
        "#   !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M-subword.vec.zip\n",
        "#   !unzip wiki*.zip\n",
        "#   #To move all .txt from colab to drive files, but not folders:\n",
        "#   !mv -v /content/*.vec '/content/drive/My Drive/Data/fastText'\n",
        "\n",
        "# download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fqL29mglAjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "BEAM_LENGTH = 1\n",
        "BEAM_COUNT_EACH_TIME_STEP = 1\n",
        "\n",
        "def beamSearch (previousTokenIndex, previousDecoderState, cumulativeSentenceProbability, beamLength):\n",
        "    \n",
        "    #stopping conditions, either local beam length is finished or we faced end token\n",
        "\n",
        "    \n",
        "    if previousTokenIndex == t.word_index[end_token]:\n",
        "        #normalize beam probability if we face unequal beam lengths due to end_token\n",
        "        averageWordProbability = 2**(log(cumulativeSentenceProbability,2)/(BEAM_LENGTH-beamLength))\n",
        "        cumulativeSentenceProbability = averageWordProbability**BEAM_LENGTH\n",
        "        previousDecoderState = None #to raise errors if used again\n",
        "        beamLength = 0#to ensure going to the second if statement\n",
        "    \n",
        "    if beamLength <= 0:\n",
        "        return ([], previousDecoderState, cumulativeSentenceProbability)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = previousTokenIndex\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + previousDecoderState)\n",
        "    sorted_indices = np.argsort(output_tokens)\n",
        "    \n",
        "    filteredIndices = np.zeros((BEAM_COUNT_EACH_TIME_STEP), dtype=int)\n",
        "    filteredProbabilities = np.zeros((BEAM_COUNT_EACH_TIME_STEP))\n",
        "    for beamLocalID in range(BEAM_COUNT_EACH_TIME_STEP):\n",
        "        filteredIndices[beamLocalID] = sorted_indices[0,0,-(1+beamLocalID)]\n",
        "        filteredProbabilities[beamLocalID] = output_tokens[0,-1,filteredIndices[beamLocalID]]\n",
        "        \n",
        "    \n",
        "    #delete to save memory on recursion\n",
        "    del output_tokens \n",
        "    del previousDecoderState\n",
        "    \n",
        "    maximumProbabilitySentenceIndices = []\n",
        "    maximumProbabilityBeamLastDecoderState = None\n",
        "    maximumProbabilityBeamCumulativeSentenceProbability = -66666666\n",
        "    \n",
        "    bestTokenIndex = -1\n",
        "    \n",
        "    for beamLocalID in range(BEAM_COUNT_EACH_TIME_STEP):\n",
        "        current_token_index = filteredIndices[beamLocalID]\n",
        "        current_token_index_prediction_probability = filteredProbabilities[beamLocalID]\n",
        "        sentenceIndices,lastDecoderState,currentCumulativeSentenceProbability = beamSearch(current_token_index,[h, c],cumulativeSentenceProbability*current_token_index_prediction_probability*10,beamLength-1)\n",
        "        if currentCumulativeSentenceProbability > maximumProbabilityBeamCumulativeSentenceProbability:\n",
        "            bestTokenIndex = current_token_index\n",
        "            maximumProbabilityBeamLastDecoderState = lastDecoderState\n",
        "            maximumProbabilitySentenceIndices = sentenceIndices\n",
        "            maximumProbabilityBeamCumulativeSentenceProbability = currentCumulativeSentenceProbability\n",
        "            \n",
        "    if bestTokenIndex != 0 and bestTokenIndex != t.word_index[end_token]:\n",
        "        sentenceIndices.insert(0,bestTokenIndex)\n",
        "    return (sentenceIndices,maximumProbabilityBeamLastDecoderState,maximumProbabilityBeamCumulativeSentenceProbability)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UMx8jcQDlxrb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence_with_beamsearch(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    sentenceIndices = []\n",
        "    cumulativeSentenceProbability = 1\n",
        "    start_token_index = t.word_index[start_token]\n",
        "    \n",
        "    for number in range(0,max_decoder_seq_length,BEAM_LENGTH):\n",
        "        if states_value is None:\n",
        "            break\n",
        "        partialSentence, states_value, currentCumulativeSentenceProbability = beamSearch(start_token_index, states_value, 1, min(BEAM_LENGTH, max_decoder_seq_length-number))\n",
        "        if(len(partialSentence)>0):\n",
        "            start_token_index = partialSentence[-1]\n",
        "        cumulativeSentenceProbability *= currentCumulativeSentenceProbability\n",
        "        sentenceIndices = sentenceIndices + partialSentence\n",
        "        \n",
        "    decodedSentence = ''\n",
        "    for index in sentenceIndices:\n",
        "        if index == 0:\n",
        "            continue\n",
        "        decodedSentence += t.index_word[index] + ' '\n",
        "        \n",
        "    return decodedSentence\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}